# Experiment 1

```{r e2-read-data}
e2_color_data <- read_csv(here("data/exp2/color.csv")) %>%
  mutate(condition = "color", targetsize = "big") %>%
  rename(adj = colorasked, distractorfeature = distractorcolor)

e2_size_data <- read_csv(here("data/exp2/size.csv")) %>%
  mutate(condition = "size") %>%
  rename(adj = sizeasked, distractorfeature = distractorsize)

e2_data <- rbind(e2_color_data, e2_size_data) %>%
  mutate(subid = paste0(subid, condition))

e2_keep_subjs <- e2_data %>%
  filter(searchtype == "attncheck", attncheckscore >= 6) %>%
  group_by(subid) 

e2_kept_n <- e2_keep_subjs %>% distinct(subid) %>% nrow()

e2_excluded_n <- e2_data %>% distinct(subid) %>% nrow() - e2_kept_n

e2_model_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  filter(trialtype != 0)

e2_subj_data <- e2_data %>%
  filter(subid %in% e2_keep_subjs$subid) %>%
  filter(trialtype != 0) %>%
  rename(adjective = adj) %>%
  mutate(searchtype = if_else(searchtype == "polychrome" | 
                                searchtype == "differentsizes",
                                      "different", searchtype)) %>%
  mutate(searchtype = if_else(searchtype == "monochrome" |
                                searchtype == "samesize",
                                      "same", searchtype)) %>%
  mutate(rtsearch = rtsearch - 6500) %>%
  mutate(log_rt = log(rtsearch)) %>%
  mutate(adjective = if_else(adjective == TRUE, "adjective noun", "noun"),
         adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("different", "contrast", "same")))

e2_mean_data <- e2_subj_data %>%
  group_by(searchtype, adjective, condition) %>%
  tidyboot_mean(percentage)
```

When referring to a *big red dog* or a *hot-air balloon*, we often take care to describe themâ€”even when there are no other dogs or balloons around. Speakers use more description when referring to objects with atypical features (e.g., a yellow tomato) than typical ones [e.g., a red tomato; see Chapter 1 and @mitchell_2013; @westerbeek_2015; @rubio-fernandez_how_2016; @bergey_morris_2020]. This selective marking of atypical objects potentially supplies useful information to listeners: they have the opportunity to not only learn about the object at hand, but also about its broader category. @horowitz_childrens_2016 demonstrated that, combined with other contrastive cues (e.g., "Wow, this one is a zib. This one is a TALL zib"), prenominal adjectives prompted adults and children to infer that the described referent was less typical than one that differed on the mentioned feature (e.g., a shorter zib). In Chapter 2, we test whether listeners use descriptive contrast with a novel object's category to learn about the category's feature distribution. 

If listeners do make contrastive inferences about typicality, it may not be as simple as judging that an described referent is atypical. Description can serve many purposes. If a descriptor was needed to distinguish between two present objects, it may not have been used to mark atypicality. For instance, in the context of a bin of heirloom tomatoes, a speaker who wanted a red one in particular might specify that they want a "red tomato" rather than just asking for a "tomato." In this case, the adjective "red" is being used contrastively with respect to reference, and not to mark atypicality. Thus, a listener who does not know much about tomatoes may attribute the use of "red" to referential disambiguation given the context and not infer that red is an unusual color for tomatoes.

In this experiment, we used an artificial language task to set up just this kind of learning situation. We manipulated the contexts in which listeners hear adjectives modifying novel names of novel referents. These contexts varied in how useful the adjective was to identify the referent: in one context the adjective was necessary, in another it was helpful, and in a third it was entirely redundant. On a reference-first view, use of an adjective that was necessary for reference can be explained away and should not prompt further inferences about typicality--an atypicality inference would be blocked. If, on the other hand, people take into account speakers' multiple reasons for using adjectives without giving priority to reference, they may alter their inferences about typicality across these contexts in a graded way: if an adjective was necessary for reference, it may prompt slightly weaker inferences of atypicality; if an adjective was redundant with respect to reference, it may be inferred to mark atypicality more strongly. Further, these contexts may also prompt distinct inferences when no adjective is used: for instance, when an adjective is necessary to identify the referent but elided, people may infer that the elided feature is particularly typical. To account for the multiple ways context effects might emerge, we analyze both of these possibilities. Overall, we asked whether listeners infer that these adjectives identify atypical features of the named objects, and whether the strength of this inference depends on the referential ambiguity of the context in which adjectives are used.

```{r e2-aliens, fig.cap = "Experiment 2 stimuli. In the above example, the critical feature is size and the object context is a within-category contrast: the alien on the right has two same-shaped objects that differ in size."}
img <- png::readPNG(here("figs/e2-stimuli.png"))
grid::grid.raster(img)
```

## Method

### Participants.

240 participants were recruited from Amazon Mechanical Turk. Half of the participants were assigned to a condition in which the critical feature was color (red, blue, purple, or green), and the other half of participants were assigned to a condition in which the critical feature was size (small or big).

### Stimuli & Procedure.

Stimulus displays showed two alien interlocutors, one on the left side (Alien A) and one on the right side (Alien B) of the screen, each with two novel fruit objects beneath them (Figure \ref{fig:e2-aliens}). Alien A, in a speech bubble, asked Alien B for one of its fruits (e.g., "Hey, pass me the big toma"). Alien B replied, "Here you go!" and the referent disappeared from Alien B's side and reappeared on Alien A's side. 

We manipulated the critical feature type (color or size) between subjects. Two factors (presence of the critical adjective in the referring expression and object context) were fully crossed within subjects. Object context had three levels: within-category contrast, between-category contrast, and same feature (Figure \ref{fig:e2-results}). In the within-category contrast condition, Alien B possessed the target object and another object of the same shape, but with a different value of the critical feature (e.g., a big toma and a small toma). In the between-category contrast condition, Alien B possessed the target object and another object of a different shape, and with a different value of the critical feature (e.g., a big toma and a small blicket). In the same feature condition, Alien B possessed the target object and another object of a different shape but with the same value of the critical feature as the target (e.g., a big toma and a big dax). Thus, in the within-category contrast condition, the descriptor was necessary to distinguish the referent; in the between-category contrast condition it was unnecessary but potentially helpful; and in the same feature condition it was unnecessary and unhelpful. 

Note that in all context conditions, the set of objects on screen was the same in terms of the experiment design: there was a target (e.g., big toma), an object with the same shape as the target and a different critical feature (e.g., small toma), an object with a different shape from the target and the same critical feature (e.g., big dax), and an object with a different shape from the target and a different critical feature (e.g., small blicket). Context was manipulated by rearranging these objects such that the relevant referents (the objects under Alien B) differed and the remaining objects were under Alien A. Thus, in each case, participants saw the target object and one other object that shared the target object's shape but not its critical feature--they observed the same kind of feature distribution of the target object's category in each trial type. 

The particular values of the features were chosen randomly for each trial, and fruits were chosen randomly at each trial from 25 fruit kinds. Ten of the 25 fruit drawings were adapted and redrawn from @kanwisher; we designed the remaining 15 fruit kinds. Each fruit kind had an instance in each of four colors (red, blue, green, or purple) and two sizes (big or small).

Participants completed six trials. After each exchange between the alien interlocutors, they made a judgment about the prevalence of the target's critical feature in the target object's category. For instance, after seeing a red blicket being exchanged, participants would be asked, "On this planet, what percentage of blickets do you think are red?" They would answer on a sliding scale between zero and 100. In the size condition, participants were asked, "On this planet, what percentage of blickets do you think are the size shown below?" with an image of the target object they just saw available on the screen. 

After completing the study, participants were asked to select which of a set of alien words they had seen previously during the study. Four were words they had seen, and four were novel lure words. Participants were dropped from further analysis if they did not respond to at least 6 of these 8 correctly (above chance performance as indicated by a one-tailed binomial test at the $p = .05$ level). This resulted in excluding `r e2_excluded_n` participants, leaving `r e2_kept_n` for further analysis.  

## Results

```{r e2-models}
e2_model <- lmer(percentage ~ condition * adjective * searchtype +
                (adjective | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e2_adj", "e2_size", "e2_search_contrast", "e2_search_same", "e2_adj_contrast", "e2_adj_same"), 
      c("adjectiveadjective noun", "conditionsize", "searchtypecontrast", "searchtypesame",
        "adjectiveadjective noun:searchtypecontrast", "adjectiveadjective noun:searchtypesame"), 
      ~ make_text_vars(e2_model, .x, .y))

e2_model_all_adj <- lmer(percentage ~ condition * searchtype +
                (1 | subid),
              control = lmerControl(optimizer = "bobyqa"),
              data = e2_subj_data %>% filter(adjective == "adjective noun")) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e2_all_adj_search_contrast", "e2_all_adj_search_same"), 
      c("searchtypecontrast", "searchtypesame"), 
      ~ make_text_vars(e2_model_all_adj, .x, .y))
```

```{r e2-results, fig.cap = "Prevalence judgments from Experiment 2. Participants consistently judged the target object as less typical of its category when the referent was described with an adjective (e.g., \"Pass me the blue toma\") than when it was not (e.g., \"Pass me the toma\"). This inference was not significantly modulated by object context (examples shown above each figure panel).", fig.height = 4, fig.width = 6.46}
labels <- rasterGrob(png::readPNG(here("figs/e2_plot_labels.png")), interpolate = TRUE)


plot <- ggplot(e2_mean_data %>% mutate(searchtype = factor(searchtype, 
                                                           levels = c("contrast", "different", "same"))),
       aes(x = adjective, color = condition, group = condition)) +
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper), 
                      position = position_dodge(.5)) +
  facet_wrap(~searchtype) +
  ylab("Prevalence judgment") +
  xlab("Utterance type") +
  labs(color = "Adjective type") + 
  scale_color_ptol() +
  geom_dl(aes(label = condition, y = empirical_stat), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 1, y = y - .7), "first.points", cex=.7)) +
theme(
  strip.background = element_blank(),
  strip.text.x = element_blank(),
  legend.position = "none")


grid.arrange(labels, plot, nrow = 2,
             widths = c(1,19),
             heights = c(2,5),
             layout_matrix = rbind(c(NA, 1),
                                   c(2)))


```

Our key test is whether participants infer that a mentioned feature is less typical than one that is not mentioned. In addition, we tested whether inferences of atypicality are modulated by context. One way to test this is to analyze the interaction between utterance type and context, seeing if the difference between adjective and no adjective utterances is larger when the adjective was highly redundant or smaller when the adjective was necessary for reference.

We analyzed participants' judgments of the prevalence of the target object's critical feature in its category. We began by fitting a maximum mixed-effects linear model with effects of utterance type (adjective or no adjective), context type (within category, between category, or same feature, with between category as the reference level), and critical feature (color or size) as well as all interactions and random slopes of utterance type and context type nested within subject. Random effects were removed until the model converged. The final model included the effects of utterance type, context type, and critical feature and their interactions, and a random slope of utterance type by subject. 

This model revealed a significant effect of utterance type ($\beta_{adjective} =$ `r e2_adj_estimate`, $t =$ `r e2_adj_statistic`, $p =$ `r e2_adj_p.value`), such that prevalence judgments were lower when an adjective was used than when it was not. Participants' inferences did not significantly differ between color and size adjective conditions ($\beta_{size} =$ `r e2_size_estimate`, $t =$ `r e2_size_statistic`, $p =$ `r e2_size_p.value`). Participants' inferences did not significantly vary by context type ($\beta_{within} =$ `r e2_search_contrast_estimate`, $t =$ `r e2_search_contrast_statistic`, $p =$ `r e2_search_contrast_p.value`; $\beta_{same} =$ `r e2_search_same_estimate`, $t =$ `r e2_search_same_statistic`, $p =$ `r e2_search_same_p.value`). There was not a significant interaction between context and presence of an adjective in the utterance ($\beta_{within*adjective} =$ `r e2_adj_contrast_estimate`, $t =$ `r e2_adj_contrast_statistic`, $p =$ `r e2_adj_contrast_p.value`; $\beta_{same*adjective} =$ `r e2_adj_same_estimate`, $t =$ `r e2_adj_same_statistic`, $p =$ `r e2_adj_same_p.value`). That is, participants did not significantly adjust their inferences based on object context, nor did they make differential inferences based on the combination of context and adjective use. However, they robustly inferred that mentioned features were less prevalent in the targetâ€™s category than unmentioned features.  

This lack of a context effect may be because people do not take context into account, or because they make distinct inferences when an adjective is *not* used: for instance, when an adjective is necessary for reference but elided, people may infer that the unmentioned feature is very typical. This inference would lead to a difference between the adjective and no adjective utterances in the within-category context, but not because people are failing to attribute the adjective to reference. To account for this possibility, we additionally tested for differences in the context conditions among only the utterances with adjectives. We fit a model with effects of context type and critical feature as well as their interaction and random slopes by subject. Participants did not significantly adjust their inferences by context among only the adjective utterances ($\beta_{within} =$ `r e2_all_adj_search_contrast_estimate`, $t =$ `r e2_all_adj_search_contrast_statistic`, $p =$ `r e2_all_adj_search_contrast_p.value`; $\beta_{same} =$ `r e2_all_adj_search_same_estimate`, $t =$ `r e2_all_adj_search_same_statistic`, $p =$ `r e2_all_adj_search_same_p.value`). Thus, even by this more specific test, participants did not adjust their inferences based on the referential context.

## Discussion

Description is often used not to distinguish among present objects, but to pick out an object's feature as atypical of its category. In Experiment 1, we asked whether people would infer that a described feature is atypical of a novel category after hearing it mentioned in an exchange. We found that people robustly inferred that a mentioned feature was atypical of its category, across both size and color description. Further, participants did not use object context to substantially explain away description. That is, even when description was necessary to distinguish among present objects (e.g., there were two same-shaped objects that differed only in the mentioned feature), participants still inferred that the feature was atypical of its category. This suggests that, in the case of hearing someone ask for a "red tomato" from a bin of many-colored heirloom tomatoes, a person naive about tomatoes would infer that tomatoes are relatively unlikely to be red.

## Model

To formalize the inference that participants were asked to make, we developed a model in the Rational Speech Act Framework  [RSA, @frank2012]. In this framework, pragmatic listeners ($L$) are modeled as drawing inferences about speakers' ($S$) communicative intentions in talking to a hypothetical literal listener ($L_{0}$). This literal listener makes no pragmatic inferences at all, evaluating the literal truth of a statement (e.g., it is true that a red toma can be called "toma" and "red toma" but not "blue toma"), and chooses randomly among all referents consistent with that statement. In planning their referring expressions, speakers choose utterances that are successful at accomplishing two goals: (1) making the listener as likely as possible to select the correct object, and (2) minimizing their communicative cost (i.e., producing as few words as possible). Note that though determiners are not given in the model's utterances, the assumption that the utterance refers to a specific reference is built into the model structure, consistent with the definite determiners used in the task. Pragmatic listeners use Bayes' rule to invert the speaker's utility function, essentially inferring what the speaker's intention was likely to be given the utterance they produced. 

To allow the Rational Speech Act Framework to capture inferences about typicality, we modified the Speaker's utility function to have an additional term: the listener's expected processing difficulty. Speakers may be motivated to help listeners to select the correct referent not just eventually but as quickly as possible. People are both slower and less accurate at identifying atypical members of a category as members of that category [@rosch_structural_1976; @dale_graded_2007]. If speakers account for listeners' processing difficulties, they should be unlikely to produce bare nouns to refer to low typicality exemplars (e.g. unlikely to call a purple carrot simply "carrot"). This is roughly the kind of inference encoded in @degen_when_2020's continuous semantics Rational Speech Act model. 

We model the speaker as reasoning about the listener's label verification process. Because the speed of verification scales with the typicality of a referent, a natural way of modeling it is as a process of searching for that particular referent in the set of all exemplars of the named category, or alternatively of sampling that particular referent from the set of all exemplars in that category, $P\left(r \vert Cat\right)$. On this account, speakers want to provide a modifying adjective for atypical referents because the probability of sampling them from their category is low, but the probability of sampling them from the modified category is much higher (a generalization of the size principle [@xu2007]). Typicality is just one term in the speaker's utility, and thus is directly weighed with the literal listener's judgment and against cost.

```{r}
# (e.g., $P\left(\text{yellow_tomato_i} \vert \text{tomatoes}\right) < P\left(\text{yellow_tomato_i} \vert \text{yellow_tomatoes}\right)$)
```

If speakers use this utility function, a listener who does not know the feature distribution for a category can use a speaker's utterance to infer it. Intuitively, a speaker should prefer not to modify nouns with adjectives because they incur a cost for producing an extra word. If they did use an adjective, it must be because they thought the learner would have a difficult time finding the referent from a bare noun alone because of typicality, competing referents, or both. To infer the true prevalence of the target feature in the category, learners combine the speaker's utterance with their prior beliefs about the feature distribution. We model the learner's prior about the prevalance of features in any category as a $\text{Beta}$ distribution with two parameters $\alpha$ and $\beta$ that encode the number of hypothesized prior psuedo-exemplars with the feature and without feature that the learner has previously observed (e.g., one red dax and one blue dax). We assume that the learner believes they have previously observed one hypothetical psuedo-examplar of each type, which is a weak symmetric prior indicating that the learner expects features to occur in half of all members of a category on average, but would find many levels of prevalence  unsurprising. To model the learner's direct experience with the category, we add the observed instances in the experiment to these hypothesized prior instances. After observing one member of the target category with the relevant feature and one without, the listener's prior is thus updated to be $\text{Beta}\left(2,\,2\right)$.

```{r e2-webppl-markedness, eval = FALSE}
markedness_utterances <- tibble(utterance = c("toma", "red toma"),
                                utterance_num = as.character(1:2))

markedness_inference <- map_dfr(markedness_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/markedness.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(markedness_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble() %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")))

write_csv(markedness_inference, here("webppl/model_estimates/markedness.csv"))
```

```{r load-e2-webppl-markedness}
markedness_inference <- read_csv(here("webppl/model_estimates/markedness.csv"),
                                 show_col_types = FALSE)
```

```{r summarise-markedness}
markedness_means <- markedness_inference %>%
  group_by(utterance) %>%
  summarise(value = mean(value))
```

```{r e2-markedness, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "Model estimates of typicality judgments for one object seen alone and labeled either [noun] or [adjective noun].", eval = FALSE}
ggplot(markedness_inference, aes(x = utterance, y = value)) + 
  geom_violin() +
  scale_y_continuous(limits = c(0, 1)) +
  geom_crossbar(aes(ymin = value, ymax = value, y = value), 
                data = markedness_means, size = .5) + 
  labs(x = "utterance", y = "proportion of [nouns] that are [adjective]")
```


```{r estimate-e2-params, eval = FALSE}
e2_estimation_data <- e2_subj_data %>%
  select(searchtype, adjective, condition, percentage) %>%
  mutate(utt = if_else(adjective == "adjective noun", "red toma", "toma")) %>%
  rowwise() %>%
  mutate(p = min(max(percentage, 1),99)) %>%
  ungroup()

e2_size_estimation_data <- e2_estimation_data %>%
  filter(condition == "size") 

e2_color_estimation_data <- e2_estimation_data %>%
  filter(condition == "color") 

e2_size_parameter_samples <- webppl(program_file =
                           here("webppl/infer_e2_params.wppl"), 
                          data = e2_size_estimation_data,
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

e2_color_parameter_samples <- webppl(program_file =
                           here("webppl/infer_e2_params.wppl"), 
                          data = e2_color_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

e2_parameters <- e2_size_parameter_samples %>%
  bind_rows(e2_color_parameter_samples)

write_csv(e2_parameters, here("webppl/model_parameters/e2_parameters.csv"))
```

```{r load-e2-params}
e2_parameters <- read_csv(here("webppl/model_parameters/e2_parameters.csv"),
                          show_col_types = FALSE)
```

```{r summarize-e2-params}
e2_parameter_means <- e2_parameters %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

e2_color_parameter <- e2_parameter_means %>%
  filter(parameter == "color")

e2_size_parameter <- e2_parameter_means %>%
  filter(parameter == "size")
```

```{r plot-e2-params, eval = FALSE}
ggplot(e2_parameters, aes(x = value, fill = parameter)) +
  facet_grid(parameter ~ .) +
  geom_histogram() + 
  theme(legend.position = c(.8, .8))
```

We used Bayesian data analysis to estimate the posterior mean rationality parameter that participants are using to draw inferences about speakers in both the color and size conditions. The absolute values of these parameters are driven largely by the number of pseudo-exemplars assumed by the listener prior to exposure; however, differences between color and size within the model are interpretable. We found that listeners inferred speakers to be directionally more rational when using size adjectives (`r e2_size_parameter$mean` [`r e2_size_parameter$ci_lower`, `r e2_size_parameter$ci_upper`]) than color adjectives (`r e2_color_parameter$mean` [`r e2_color_parameter$ci_lower`, `r e2_color_parameter$ci_upper`]), but the two inferred confidence intervals were overlapping, suggesting that people treated size and color adjectives similarly when making inferences about typicality.

```{r e2-empirical, eval = FALSE}
e2_utterances <- expand_grid(utterance = c("toma", "red toma"),
                             type = c("contrast", "same", "different"),
                             parameter = c("size", "color")) %>%
  mutate(utterance_num = if_else(utterance == "toma", 1, 2)) %>%
  left_join(e2_parameter_means, by = "parameter")
 

e2_inference <- e2_utterances %>%
  group_by(utterance_num, parameter, type) %>%
  nest() %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here(glue("webppl/e2_{type}_empirical.wppl")),
                                       data = .x))) %>%
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(e2_utterances, by = c("utterance_num", "type", "parameter")) %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "toma", "noun", "adjective noun"),
         utterance = factor(utterance, 
                            levels = c("noun", "adjective noun"))) 
  
write_csv(e2_inference, here("webppl/model_estimates/e2_estimates.csv"))
```

```{r load-e2-estimates}
e2_inference <- read_csv(here("webppl/model_estimates/e2_estimates.csv"),
                         show_col_types = FALSE)
```

```{r summarise-e2-estimates}
e2_inference_means <- e2_inference %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  mutate(p = as.numeric(p)) %>%
  filter(obj == "red toma", world == "target world") %>%
  group_by(utterance, type, parameter) %>%
  summarise(p = mean(p))
```

```{r e2-wppl-plot, fig.height = 4, fig.width = 6.46, fig.cap = "Participants' prevalence judgments from Experiment 2, compared to model predictions (horizontal lines)."}
e2_wppl_data <- e2_inference_means %>%
  mutate(p = 100 * p) %>%
  rename(empirical_stat = p, adjective = utterance,
         searchtype = type, condition = parameter) %>%
  mutate(adjective = factor(adjective, levels = c("noun", "adjective noun")),
         searchtype = factor(searchtype, levels = c("contrast", "different", "same")))
# 
# ggplot(e2_mean_data, aes(x = searchtype, y = empirical_stat - 50, 
#                          color = adjective, fill = adjective)) + 
#   facet_wrap(~ condition) +
#   geom_pointrange(aes(ymin = ci_lower - 50, ymax = ci_upper - 50), 
#                   position = position_dodge(.5)) + 
#   theme(legend.position = "top") + 
#   geom_col(data = e2_wppl_data, width = .5, 
#            position = position_dodge(.5), alpha = .5) + 
#   geom_hline(aes(yintercept = 0), linetype = "dashed") +
#   scale_y_continuous(labels = function(y) y + 50, limits = c(-15,15))

plot_model2 <- ggplot(e2_mean_data %>% mutate(searchtype = factor(searchtype, 
                                                           levels = c("contrast", "different", "same"))),
                      aes(x = adjective, y = empirical_stat, 
                         color = condition)) + 
  facet_wrap(~ searchtype) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(.5)) + 
  ylab("Prevalence judgment") +
  xlab("Utterance type") +
  labs(color = "Adjective type") + 
  geom_crossbar(aes(ymin = empirical_stat, ymax = empirical_stat, y = empirical_stat),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5, data = e2_wppl_data) + 
  geom_hline(aes(yintercept = 50), linetype = "dashed") +
  scale_color_ptol() +
  geom_dl(aes(label = condition), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 1, y = y - .7), "first.points", cex=.7)) +
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

grid.arrange(labels, plot_model2, nrow = 2,
             widths = c(1,19),
             heights = c(2,5),
             layout_matrix = rbind(c(NA, 1),
                                   c(2)))
```


Figure \ref{fig:e2-wppl-plot} shows the predictions of our Rational Speech Act model compared to empirical data from participants. The model captures the trends in the data correctly, inferring that the critical feature was less prevalent in the category when it was mentioned (e.g., "red dax") than when it was not mentioned (e.g., "dax"). The model also infers the prevalence of the critical feature to be numerically higher in the within-category condition, like people do. That is, in the within-category condition when an adjective is used to distinguish between referents, the model thinks that the target color is slightly less atypical. When an adjective would be useful to distinguish between two objects of the same shape but one is not used, the model infers that the color of the target object is slightly more typical. 

Overall, our model captures the inference people make: when the speaker mentions a feature (e.g., "the blue dax"), that feature is inferred to be less typical of the category (daxes are less likely to be blue in general). It further captures that when the object context requires an adjective for successful reference, people weaken this atypicality inference only slightly, if at all. In contrast to a reference-first view, which predicts that these two kinds of inferences would trade off strongly--that is, using an adjective that is necessary for reference blocks the inference that it is marking atypicality--the model captures the graded way in which people consider these two communicative goals. 

<!--In contrast to the reference-first view that these two kinds of inferences trade off strongly--that is, adjectives are used primarily for reference, and such use blocks the inference that they are marking typicality--the model captures the graded way in which people interpolate between them. When an adjective is helpful for reference, whether it is used or not makes both the model and people give it slightly less weight in inferring the typical features of the target object, but the weight is still significant. Our model's explanation for this is that while people choose their language in order to refer successfully, their choices also reflect their knowledge of features of those objects. In the model as constructed, we cannot distinguish between listener and speaker design explanations for the impact of feature knowledge. One possibility is that the pressure from this feature knowledge is communicative: speakers could be intentionally transmitting information to the listener about the typical features of their intended referent. Alternatively, the influence of this feature knowledge could be unintentional, driven by pressures from the speaker's semantic representation. We consider these implications more fully in the General Discussion. In either case, listeners can leverage the impact of speakers' feature knowledge on their productions in order to infer the typical features of the objects they are talking about, even if this is their first exposure to these novel objects.-->

```{r joint-inference, eval = FALSE}
joint_utterances <- tibble(utterance = c("toma", "blue toma"),
                                utterance_num = as.character(1:2))

joint_inference <- map_dfr(joint_utterances %>% pull(utterance), 
                               ~webppl(program_file = 
                                         here("webppl/two_world_typicality.wppl"), 
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(joint_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  as_tibble()
```

```{r joint-analysis, eval = FALSE}
joint_data <- joint_inference %>%
  pivot_wider(names_from = "Parameter") %>%
  mutate(chosen = case_when(world == "two toma" ~ 
                           gsub("toma", "pair", obj),
                         world == "two dax" ~ 
                           gsub("toma", "single", obj))) %>%
  mutate(p = as.numeric(p),
         utterance = factor(utterance, 
                            levels = c("toma", "blue toma")))

joint_data_obj <- joint_data %>%
  group_by(utterance, chosen) %>%
  count() %>%
  group_by(utterance) %>%
  mutate(prob = n/sum(n))

ggplot(joint_data_obj, aes(x = chosen, y = prob, fill = chosen)) + 
  geom_col(position = "dodge") + 
  facet_wrap(~ utterance) + 
  scale_fill_ptol(drop = FALSE) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "", y = "selection probability")

ggplot(joint_data, aes(x = p, y = chosen)) +
  facet_wrap(~ utterance) +
  stat_density_ridges(scale = 1, 
                      quantile_lines = TRUE, quantiles = 2) + 
  scale_fill_ptol() +
  scale_x_continuous(limits = c(0, 1)) +
  labs(y = "object chosen", x = "proportion of tomas that are blue")


joint_data_subset <- joint_data %>%
  filter(!utterance %in% c("red dax"), chosen == "blue pair") %>%
  group_by(utterance, chosen) %>%
  summarise(p = mean(p)) %>%
  spread(utterance, p)

```