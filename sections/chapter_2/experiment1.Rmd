# Experiment 1   

In Experiment 1, we ask whether people use descriptive contrast to identify the target of an ambiguous referring expression. Our experiment was inspired by work from @sedivy_achieving_1999 showing that people can use contrastive inferences to guide their attention to referents as utterances progress. In their task, participants saw displays of four objects: a target (e.g., a tall cup), a contrastive pair (e.g., a short cup), a competitor that shares the target’s feature but not category (e.g., a tall pitcher), and an irrelevant distractor (e.g., a key). Participants then heard a referring expression: "Pick up the tall cup." Participants looked more quickly to the correct object when the utterance referred to an object with a same-category contrastive pair (tall cup vs. short cup) than when it referred to an object without a contrastive pair (e.g., when there was no short cup in the display). 

Their results suggest that listeners expect speakers to use prenominal description when they are distinguishing between potential referents of the same type, and listeners use this inference to rapidly allocate their attention to the target as an utterance progresses. This principle does not apply equally across adjective types, however: color adjectives seem to hold less contrastive weight [@sedivy_pragmatic_2003], perhaps because color adjectives are often used redundantly in English--that is, people describe objects' colors even when this description is not necessary to establish reference [@pechmann_incremental_1989]. @kreiss2020production demonstrate that listeners' familiar referent choices closely conform to speakers' production norms, such that over-specified modifiers hold less contrastive weight. If this generalizes to novel object choice, we should find that size adjectives prompt stronger contrastive inferences than color adjectives.

In a pre-registered referential disambiguation task, we presented participants with arrays of novel fruit objects. On critical trials, participants saw a target object, a lure object that shared the target's critical feature but not its shape, and a contrastive pair that shared the target's shape but not its critical feature (Fig. \ref{fig:colortrial}). Participants heard an utterance, sometimes mentioning the critical feature: "Find the [blue/big] toma." In all trials, utterances used the definite determiner "the," which conveys that there is a specific referent to be identified. For the target object, which had a same-shaped counterpart, use of the adjective was necessary to establish reference. For the lure, which was unique in shape, the adjective was relatively superfluous description. If participants use contrastive inference to choose novel referents, they should choose the target object more often than the lure. To examine whether contrast occurs across adjective types, we tested participants in two conditions: color contrast and size contrast. Though we expected participants to shift toward choosing the item with a contrastive pair in both conditions, we did not expect them to treat color and size equally. Because color is often used redundantly in English while size is not, we expected size to hold more contrastive weight, encouraging a more consistent contrastive inference [@pechmann_incremental_1989]. The pre-registration of our method, recruitment plan, exclusion criteria, and analyses can be found on the Open Science Framework here: https://osf.io/pqkfy .


```{r colortrial, fig.env = "figure", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "On the left: an example of a contrastive trial in which the critical feature is size. Here, the participant would hear the instruction ``Find the small toma.'' On the right: an example of a contrastive trial in which the critical feature is color. Here, the participant would hear the instruction ``Find the red toma.'' In both cases, the target is the top object."}
img <- png::readPNG(here("figs/sizecolorcontrast.png"))
grid::grid.raster(img)
```


## Method

```{r load-data}
e1_raw_data <- read_csv(here("data/exp1_turk_data.csv")) 

e1_total_subjs <- e1_raw_data %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_color_subjs <- e1_raw_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  count() %>%
  pull()

e1_total_size_subjs <- e1_raw_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  count() %>%
  pull()


e1_keep_subjs <- e1_raw_data %>%
  filter(searchtype == "colorcheck", chosetarget == TRUE, 
         attncheckscore >= 6) %>%
  count(subid) %>%
  filter(n == 4) %>%
  pull(subid)

e1_data_no_gather <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  mutate(subid = as.factor(subid))

e1_data <- e1_raw_data %>%
  filter(subid %in% e1_keep_subjs,
         trialtype != 0) %>%
  pivot_longer(cols = c(chosetarget, choselure, choseunique), 
               names_to = "item", values_to = "chose") %>%
  mutate(item = gsub("chose", "", item),
         subid = as.factor(subid))

e1_color_subjs <- e1_data %>%
  filter(condition == "color") %>%
  distinct(subid) %>%
  nrow()

e1_size_subjs <- e1_data %>%
  filter(condition == "size") %>%
  distinct(subid) %>%
  nrow()

e1_mean_data <- e1_data %>%
  filter(item != "unique") %>%
  group_by(condition, searchtype, adj, item, subid) %>%
  summarise(chose = mean(chose)) %>%
  tidyboot_mean(chose) %>%
  ungroup() %>%
  mutate(adjective_used = factor(adj, labels = c("noun", "adjective noun"))) 
```

### Participants.

We recruited a pre-registered sample of `r e1_total_subjs` participants through Amazon Mechanical Turk. Half of the participants were assigned to a condition in which the critical feature was color (stimuli contrasted on color), and the other half were assigned to a condition in which the critical feature was size. Each participant gave informed consent and was paid $0.30 in exchange for their participation.
  

### Stimuli.

Stimulus displays were arrays of three novel fruit objects. Fruits were chosen randomly at each trial from 25 fruit kinds. Ten of the 25 fruit drawings were adapted and redrawn from @kanwisher; we designed the remaining 15 fruit kinds. Each fruit kind had an instance in each of four colors (red, blue, green, or purple) and two sizes (big or small). Particular target colors were assigned randomly at each trial and particular target sizes were counterbalanced across display types.  There were two display types: unique target displays and contrastive displays. Unique target displays contained a target object that had a unique shape and was unique on the trial's critical feature (color or size), and two distractor objects that matched each other's (but not the target's) shape and critical feature. These unique target displays were included as a check that participants were making reasonable referent choices and to space out contrastive displays to prevent participants from dialing in on the contrastive object setup during the experiment. Contrastive displays contained a target, its contrastive pair (matched the target's shape but not its critical feature), and a lure (matched the target’s critical feature but not its shape; Fig. \ref{fig:colortrial}). The on-screen positions of the target and distractor items were randomized within a triad configuration.


```{r e1-fig, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Proportion of times that participants chose the target and lure items as a function of condition and whether an adjective was provided. Points indicate group means; error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping."}
condition_names <- c(
                    "contrast" = "contrastive display",
                    "uniquetarget" = "unique target display",
                    "size" = "size",
                    "color" = "color"
                    )
e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  mutate(item = factor(item, levels = c("target", "lure"))) %>%
  ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
  facet_grid(~condition, labeller = as_labeller(condition_names)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.25)) + 
  scale_color_ptol() + 
  ylab("Item chosen") + 
  xlab("Utterance type") + 
  geom_dl(method = list(dl.trans(x=x - .5), "first.qp", cex=.7)) +
  theme(legend.position = "none")

# e1_mean_data %>%
#   filter(searchtype == "contrast") %>%
#   ggplot(aes(x = adjective_used, color = item, label = item, y = empirical_stat)) +
#   facet_wrap(~condition, labeller = as_labeller(condition_names)) + 
#   geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
#                   position = position_dodge(.25)) + 
#   scale_color_ptol() + 
#   ylab("Item chosen") + 
#   xlab("") + 
#   geom_dl(method = list(dl.trans(x=x - .5), "first.qp", cex=.7)) +
#   theme(legend.position = "none", text = element_text(size=15))
```

### Design and Procedure.

Participants were told they would play a game in which they would search for strange alien fruits. Each participant saw eight trials. Half of the trials were unique target displays and half were contrastive displays. Crossed with display type, half of trials had audio instructions that described the critical feature of the target (e.g., "Find the [blue/big] toma"), and half of trials had audio instructions with no adjective description (e.g., "Find the toma"). A name was randomly chosen at each trial from a list of eight nonce names: blicket, wug, toma, gade, sprock, koba, zorp, and lomet. 

After completing the study, participants were asked to select which of a set of alien words they had heard previously during the study. Four were words they had heard, and four were novel lure words. Participants were dropped from further analysis if they did not meet our pre-registered exclusion criteria of responding to at least 6 of these 8 memory check questions correctly (above chance performance as indicated by a one-tailed binomial test at the $p = .05$ level) and answering all four color perception check trials correctly (resulting $n =$ `r length(e1_keep_subjs)`)^[Experiments 1 and 3 were run in 2020, during the COVID-19 pandemic, when high exclusion rates on Amazon Mechanical Turk were being reported by many experimenters. Though our pre-registered criteria led to many exclusions, the check given to participants tested memory for a few novel words heard in the experiment, which we do not believe was an overly stringent requirement.].

## Results

```{r e1-models}
chance_comparisons <- e1_data %>%
  filter(searchtype == "uniquetarget", item == "target") %>% 
  group_by(adj, condition, subid)

glmer_chance_comparison <- chance_comparisons %>%
  filter(!adj) %>%
  group_by(condition) %>%
  mutate(options = 3) %>%
  nest() %>%
  mutate(model = map(data, ~glmer(chose ~  (1|subid), offset = logit(1/options),
                                  family = "binomial", data = .))) %>%
  mutate(model = map(model, tidy)) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-term) %>%
  rename(term = condition) %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_color_chance", "e1_size_chance"), c("color", "size"), 
      ~ make_text_vars(glmer_chance_comparison, .x, .y))


glmer_unique <- chance_comparisons %>%
  glmer(chose ~ condition * adj + (1|subid), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

walk2(c("e1_target_size", "e1_target_adj", "e1_target_size_adj"), 
      c("conditionsize", "adjTRUE", "conditionsize:adjTRUE"), 
      ~ make_text_vars(glmer_unique, .x, .y))
```


We first confirmed that participants understood the task by analyzing performance on unique target trials, the filler trials in which there was a target unique on both shape and the relevant adjective. We asked whether participants chose the target more often than expected by chance ($33\%$) by fitting a mixed effects logistic regression with an intercept term, a random effect of subject, and an offset of $logit(1/3)$ to set chance probability to the correct level. The intercept term was reliably different from zero for both color ($\beta =$ `r e1_color_chance_estimate`, $t =$ `r e1_color_chance_statistic`, $p$ `r e1_color_chance_p.value`) and size ($\beta =$ `r e1_size_chance_estimate` , $t =$ `r e1_size_chance_statistic`, $p$ `r e1_size_chance_p.value`), indicating that participants consistently chose the unique object on the screen when given an instruction like "Find the (blue) toma." In addition, participants were more likely to select the target when an adjective was provided in the audio instruction in both conditions. We confirmed this effect statistically by fitting a mixed effects logistic regression predicting target selection from condition, adjective use, and their interaction with random effects of participants. Use of description in the audio increased target choice ($\beta =$ `r e1_target_adj_estimate`, $t =$ `r e1_target_adj_statistic`, $p$ `r e1_target_adj_p.value`), and adjective type (color vs. size) was not statistically related to target choice ($\beta =$ `r e1_target_size_estimate`, $t =$ `r e1_target_size_statistic`, $p =$ `r e1_target_size_p.value`). The two effects had a marginal interaction ($\beta =$ `r e1_target_size_adj_estimate`, $t =$ `r e1_target_size_adj_statistic`, $p =$ `r e1_target_size_adj_p.value`). Participants had a general tendency to choose the target in unique target trials, which was strengthened if the audio instruction contained the relevant adjective. These effects did not significantly differ between color and size adjectives, which suggests that participants did not treat color and size differently in these baseline trials, though the marginal interaction suggests that use of an adjective may strengthen their tendency to choose the unique object more powerfully in the size condition.


```{r e1-models-contrast}
# in contrast trials w/an adjective, do people choose the target over the lure?
# prereg'd
chance_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, 
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(chance_model, "e1_overall_contrast")


# in only *color* contrast trials w/an adjective, do people choose the target over the lure?
# not prereg'd
color_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, condition == "color",
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(color_model, "e1_color_contrast")

size_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, condition == "size",
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(size_model, "e1_size_contrast")


no_adj_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == FALSE, 
         (chosetarget == TRUE || choselure == TRUE)) %>%
  glmer(chosetarget ~ 1 + (1 | subid),
        family = "binomial", data = .) %>%
  tidy()  %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))

make_text_vars(no_adj_model, "e1_no_adj")

# in contrast trials w/an adjective, 
# does the type of adjective matter in choosing target over lure?
# prereg'd
adj_type_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast", adj == TRUE, 
         (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed", term == "conditionsize") %>%
  mutate(p.value = printp(p.value))

make_text_vars(adj_type_model, "e1_adj_type")

# in contrast trials, do adj type and presence of an adj interact 
# in determining target over lure choice?
# not prereg'd
adj_by_adjtype_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast",
         (chosetarget == TRUE | choselure == TRUE)) %>%
  glmer(chosetarget ~ condition * adj + (1 | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value))


# throw everything in the model
# prereg'd
full_model <- e1_data_no_gather %>%
  filter(searchtype == "contrast" | searchtype == "uniquetarget") %>%
  glmer(chosetarget ~ adj * condition * searchtype + (searchtype * adj | subid),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")  %>%
  mutate(p.value = printp(p.value))
```

Our key pre-registered analysis was whether participants would choose the target object on contrastive trials--when they heard an adjective in the referring expression. To perform this test, we compared participants' rate of choosing the target to their rate of choosing the lure, which shares the relevant critical feature with the target, when they heard the adjective. Overall, participants chose the target with a contrasting pair more often than the unique lure, indicating that they used contrastive inferences to resolve reference ($\beta =$ `r e1_overall_contrast_estimate`, $t =$ `r e1_overall_contrast_statistic`, $p =$ `r e1_overall_contrast_p.value`). To test whether the strength of the contrastive inference differed between color and size conditions, we pre-registered a version of this regression with a term for adjective type, and found that people were more likely to choose the target over the lure in the size condition than the color condition ($\beta =$ `r e1_adj_type_estimate`, $t =$ `r e1_adj_type_statistic`, $p =$ `r e1_adj_type_p.value`). 

Given this result, we tested whether people consistently chose the target over the lure on the color and size data separately, as a stricter check of whether the effect was present in both conditions (not pre-registered). Considering color and size separately, participants chose the target significantly more often than the lure in the size condition ($\beta =$ `r e1_size_contrast_estimate`, $t =$ `r e1_size_contrast_statistic`, $p =$ `r e1_size_contrast_p.value`), but not in the color condition ($\beta =$ `r e1_color_contrast_estimate`, $t =$ `r e1_color_contrast_statistic`, $p =$ `r e1_color_contrast_p.value`). On contrastive trials in which a descriptor was not given, participants dispreferred the target, instead choosing the lure object, which matched the target on the descriptor but had a unique shape ($\beta =$ `r e1_no_adj_estimate`, $t =$ `r e1_no_adj_statistic`, $p =$ `r e1_no_adj_p.value`). Participants' choice of the target in the size condition was therefore not due to a prior preference for the target in contrastive displays, but relied on contrastive interpretation of the adjective. In the Supplemental Materials, we report an additional pre-registered analysis of all Experiment 1 data with maximal terms and random effects; those results are consistent with the more focused tests reported here.

## Discussion

When faced with unfamiliar objects referred to by unfamiliar words, people can use pragmatic inference to resolve referential ambiguity and learn the meanings of these new words. In Experiment 1, we found that people have a general tendency to choose objects that are unique in shape when reference is ambiguous. However, when they hear an utterance with description (e.g., "blue toma", "small toma"), they shift away from choosing unique objects and toward choosing objects that have a similar contrasting counterpart. Furthermore, use of size adjectives--but not color adjectives--prompts people to choose the target object with a contrasting counterpart more often than the unique lure object. We found that people are able to use contrastive inferences about size to successfully resolve which unfamiliar object an unfamiliar word refers to. 

## Model

To formalize the inference that participants were asked to make, we developed a model in the Rational Speech Act Framework  [RSA, @frank2012]. In this framework, pragmatic listeners ($L$) are modeled as drawing inferences about speakers' ($S$) communicative intentions in talking to a hypothetical literal listener ($L_{0}$). This literal listener makes no pragmatic inferences at all, evaluating the literal truth of a statement (e.g., it is true that a red toma can be called "toma" and "red toma" but not "blue toma"), and chooses randomly among all referents consistent with that statement. In planning their referring expressions, speakers choose utterances that are successful at accomplishing two goals: (1) making the listener as likely as possible to select the correct object, and (2) minimizing their communicative cost (i.e., producing as few words as possible). Note that though determiners are not given in the model's utterances, the assumption that the utterance refers to a specific reference is built into the model structure, consistent with the definite determiners used in the task. Pragmatic listeners use Bayes' rule to invert the speaker's utility function, essentially inferring what the speaker's intention was likely to be given the utterance they produced. 

$$Literal: P_{Lit} = \delta\left(u,r\right)P\left(r\right)$$
$$Speaker: P_S\left(u \vert r\right) \propto \alpha \left(P_{Lit}\left(r \vert u\right) - C\right)$$
$$Listener: P_{Learn}\left(r \vert u\right) \propto P_s\left(u \vert r\right)P\left(r\right)$$

For this experiment, we build on a Rational Speech Act model developed by @frank2014 to jointly resolve reference and learn new words. The primary modification of RSA is use of a pragmatic *learner*: a pragmatic listener who has uncertainty about the meanings of words in their language, and thus cannot directly compute the speaker's utility as written. Instead, the speaker's utility is conditioned on the set of mappings, and the learner must also infer which set of mappings is correct:  
$$Learner: P_L\left(r \vert u\right) \propto P_s\left(u \vert r; m\right)P\left(r\right)P\left(m\right)$$

In these experiments, we assume that the prior probability to refer to each object $(P\left(r\right))$ is equal, and similarly that all mappings $(P\left(m\right))$ are equally likely, so they cancel out in computations. We further assume that the cost of producing any word is identical, and so the cost of an utterance is equal to its length. All that remains is to specify the possible mappings, and literal meanings, and alternative utterances possible on each trial of the experiment. We describe the size condition here, but the computation for the color condition is analogous. 

On the trial shown in the left panel of Figure \ref{fig:colortrial} people see two objects that look something like a hair dryer and one that looks like a pear and they are asked to "Find the toma." Here, in the experiment design and the model, we take advantage of the fact that English speakers tend to assume that nouns generally correspond to differences in shape rather than other features [@landau1992]. Given this, the two possible mappings are $\{m_1: hair dryer-``toma", pear-``?"\}$ and $\{m_2: hair dryer-``?", pear-``toma"\}$. The literal semantics of each object allow them to be referred to by their shape label (e.g. "toma"), or by a descriptor that is true of them (e.g. "small"), but not names for other shapes or untrue descriptors.

Having heard "Find the toma," the model must now choose a referent. If the true mapping for "toma" is the hair dryer ($m_1$), this utterance is ambiguous to the literal listener, as there are two referents consistent with the literal meaning toma. Consequently, whichever of the two referents the speaker intends to point out to the learner, the speaker's utility will be relatively low. Alternatively, if the true mapping for "toma" is the pear ($m_2$), then the utterance will be unambiguous to the literal listener, and thus the speaker's utterance will have higher utility. As a result, the model can infer that the more likely mapping is $m_2$ and choose the pear, simultaneously resolving reference and learning the meaning of "toma."

If instead the speaker produced "Find the small toma," the model will make a different inference. If the true mapping for "toma" is hair dryer ($m_2$), this utterance now uniquely identifies one referent for the literal listener and thus has high utility. It also uniquely identifies the target if "toma" means pear ($m_1$). However, if "toma" means pear, the speaker's utterance was inefficient because the single word utterance "toma" would have identified the target to the literal listener and incurred less cost. Thus, the model can infer that "toma" is more likely to mean hair dryer and choose the small hair dryer appropriately.

While these descriptions use deterministic language for clarity, the model's computation is probabilistic and thus reflects tendencies to choose those objects rather than fixed rules. Figure \ref{fig:e1-webppl-plot} shows model predictions alongside people's behavior for the size and color contrast conditions in Experiment 1. In line with the intuition above, the model predicts that hearing a bare noun (e.g. "toma") should lead people to infer that the intended referent is the unique object (lure), whereas hearing a modified noun (e.g. "small toma") should lead people to infer that the speaker's intended referent has a same-shaped counterpart without the described feature (i.e., is the target object). 

```{r e1-webppl, eval = FALSE}
two_world_utterances <- tibble(utterance = c("toma", "blue toma"),
                               utterance_num = as.character(1:2))

two_world_inference <- map_dfr(two_world_utterances %>% pull(utterance),
                               ~webppl(program_file =
                                         here("webppl/discrete_semantics.wppl"),
                                       data = .x),
                               .id = "utterance_num") %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 
  

e1_webppl_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(two_world_inference, by = c("utterance", "item" = "obj")) %>%
  mutate(prob_min = prob, prob_max = prob)
```

```{r estimate-e1-params, eval = FALSE}
e1_estimation_data <- e1_data_no_gather %>%
  filter(searchtype == "contrast") %>%
  mutate(choseother = !chosetarget & !choselure) %>%
  select(condition, adj, chosetarget, choselure, choseother) %>%
  mutate(utt = if_else(adj, "blue dax", "dax")) %>%
  mutate(world = case_when(chosetarget ~ "two dax",
                         choselure ~ "two toma",
                         choseother ~ "two dax",
                         TRUE ~ NA_character_),
         obj = case_when(choseother ~ "red dax",
                         chosetarget | choselure ~ "blue dax",
                         TRUE ~ NA_character_)) %>%
  filter(!is.na(world), !is.na(obj))

size_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "size")

color_estimation_data <- e1_estimation_data %>%
  filter(!(obj == "red dax" & adj)) %>%
  filter(condition == "color")

size_parameter <- webppl(program_file =
                           here("webppl/infer_discrete_semantics.wppl"), 
                          data = size_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "size")

color_parameter <- webppl(program_file =
                           here("webppl/infer_discrete_semantics.wppl"), 
                          data = color_estimation_data, 
                          data_var = "empiricalData") %>%
  as_tibble() %>%
  mutate(parameter = "color")

e1_parameters <- size_parameter %>%
  bind_rows(color_parameter)


write_csv(e1_parameters, here("webppl/model_parameters/e1_parameters.csv"))
```

```{r load-e1-params}
e1_parameters <- read_csv(here("webppl/model_parameters/e1_parameters.csv"),
                          show_col_types = FALSE)
```

```{r summarise-e1-parameters}
e1_parameter_means <- e1_parameters %>%
  group_by(parameter) %>%
  summarise(mean = mean(value),
            ci_upper = quantile(value, .975),
            ci_lower = quantile(value, .025))

e1_color_parameter <- e1_parameter_means %>% 
  filter(parameter == "color")

e1_size_parameter <- e1_parameter_means %>% 
  filter(parameter == "size")


# ggplot(e1_parameters, aes(x = value, fill = parameter)) +
#   geom_histogram(alpha = .5) + 
#   theme(legend.position = c(.8, .8))
```

```{r e1-webppl-empirical, eval = FALSE}
two_world_utterances <- tibble(utterance = c("dax", "blue dax"),
                               utterance_num = as.character(1:2))


e1_webppl_input <- two_world_utterances %>%
  mutate(parameter = "color") %>%
  bind_rows(mutate(two_world_utterances, parameter = "size")) %>%
  left_join(e1_parameter_means, by = "parameter") %>%
  group_by(utterance_num, parameter) %>%
  nest()
  
two_world_inference <- e1_webppl_input  %>%
  mutate(model_output = map(data, ~webppl(program_file =
                                         here("webppl/discrete_semantics_empirical.wppl"),
                                       data = .x))) %>%   
  select(-data) %>%
  unnest(cols = c(model_output)) %>%
  left_join(two_world_utterances, by = "utterance_num") %>%
  ungroup() %>%
  select(-utterance_num) %>%
  mutate(utterance = if_else(utterance == "dax", "noun", "adjective noun"),
         utterance = factor(utterance, levels = c("noun", "adjective noun")),
         obj = case_when(obj == "blue dax" & world_string == "two toma" ~ "lure",
                         obj == "blue dax" & world_string == "two dax" ~ "target",
                         obj != "blue dax" ~ NA_character_)) %>%
  filter(!is.na(obj)) 

write_csv(two_world_inference, here("webppl/model_estimates/e1_estimates.csv"))
```

```{r load-e1-webppl-empirical}
two_world_inference <- read_csv(here("webppl/model_estimates/e1_estimates.csv"),
                                show_col_types = FALSE)
```

```{r e1-join-data}
e1_webppl_data <- e1_mean_data %>%
  filter(searchtype == "contrast") %>%
  select(-searchtype) %>%
  rename(utterance = adjective_used) %>%
  left_join(two_world_inference, by = c("utterance", "item" = "obj", 
                                        "condition" = "parameter")) %>%
  mutate(utterance = factor(utterance, levels = c("noun", "adjective noun")),
         prob_min = prob, prob_max = prob)
```

```{r e1-webppl-plot, fig.env = "figure", fig.width=6, fig.height=3, fig.align = "center", fig.cap = "Proportion of times that people (and our model) chose the target and lure items as a function of adjective type and whether an adjective was provided. Points indicate empirical means; error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping. Solid horizontal lines indicate model predictions."}
ggplot(e1_webppl_data, aes(x = utterance, color = item, fill = item)) + 
  geom_pointrange(aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper),
                      position = position_dodge(.5),) + 
  geom_crossbar(aes(ymin = prob_min, ymax = prob_max, y = prob),
                position = position_dodge(.5), width = .5,
           alpha = .5, size = .5) + 
  facet_wrap(~ condition) + 
  labs(x = "", y = "proportion item choice") +
  scale_color_ptol() +
  scale_fill_ptol() +
  geom_dl(aes(label = item, y = empirical_stat), 
          position = position_dodge(.5),
          method = list(dl.trans(x = x + 1), "first.points", cex=.7)) +
  theme(legend.position = "none")
```

Our empirical data suggest that people treat color and size adjectives differently, making a stronger contrastive inference with size than with color. One potential explanation for this difference is that people are aware of production asymmetries between color and size. As mentioned, speakers tend to over-describe color, providing more color adjectives than necessary to establish reference, while describing size more minimally [@pechmann_incremental_1989; @nadig_evidence_2002]. Listeners may be aware of this production asymmetry and discount the contrastive weight of color adjectives with respect to reference. 

In the Rational Speech Act model, this kind of difference is captured neatly by a difference in the listener's beliefs about the speaker's rationality (i.e. how sensitive the speaker is to differences in utility of different utterances). To determine the value of the rationality parameter that best describes participants' behavior in each condition, we used Bayesian data analysis, estimating the posterior probability of the observed data under each possible value of $\alpha$ multiplied by the prior probability of each of those values. In each condition, $\alpha$ was drawn from a Gamma distribution with shape and scale parameters set to 2 ($Gamma\left(2,2\right)$). This prior encodes a weak preference for small values of $\alpha$, but the estimates below were not sensitive to other choices of hyper-parameters. 

Posterior mean estimates of rationality varied substantially across conditions. In the color condition, the rationality parameter was estimated to be `r e1_color_parameter$mean` with a 95% credible interval of [`r e1_color_parameter$ci_lower`, `r e1_color_parameter$ci_upper`]. In the size condition, rationality was estimated to be `r e1_size_parameter$mean` [`r e1_size_parameter$ci_lower`, `r e1_size_parameter$ci_upper`].

Figure \ref{fig:e1-webppl-plot} shows the model predictions along with the empirical data from Experiment 1. The model broadly captures the contrastive inference--when speakers produce an adjective noun combination like "red toma," the model selects the target object more often than the lure object. The extent to which the model makes this inference varies as predicted between the color and size adjective conditions in line with the different estimated rationality values. In both conditions, despite estimating the value of rationality that makes the observed data most probable, the model overpredicts the extent of the contrastive inference that people make. Intuitively, it appears that over and above the strength of their contrastive inferences, people have an especially strong tendency to choose a unique object when they hear an unmodified noun (e.g. "toma"). In an attempt to capture this uniqueness tendency, the model overpredicts the extent of the contrastive inference. 

The model captures the difference between color and size in a difference in the rationality parameter, but leaves open the ultimate source of this difference in rationality. Why there is a production asymmetry in the first place? For now, we bracket this question and note that listeners in our task appropriately discount color's contrastive weight given production norms. 

An alternative way to capture this preference would be to locate it in a different part of the model. One possibility is that the literal semantics of color and size work differently. A recent model from @degen_when_2020 does predict a color--size asymmetry based on different semantic exactness. In this model, literal semantics are treated as continuous rather than discrete, so "blue" is neither 100% true nor 100% false of a particular object, but can instead be 90% true. They successfully model a number of color--size asymmetries in production data by treating color as having stronger literal semantics (e.g. "blue toma" is a better description of a small blue toma than "small toma" is). However, this model predicts the opposite asymmetry of what we found. Because color has stronger semantics than size, the listener in this model shows a stronger contrast effect for color than size (see demonstration in the Supplemental Materials). Thus, though a continuous semantics can explain our asymmetry, this explanation is unlikely given that the continuous semantics that predicts other empirical color--size asymmetries does not predict our findings.

Yet another way to explain the difference between size and color adjectives is to attribute size adjectives' contrastive strength with respect to reference to the fact that size adjectives are gradable and relative. There are multiple ways to implement this possibility in the model. One way would be to specify that speakers tend to remark on relative, gradable features when making distinctions among present objects because direct comparisons for the meaning of 'small' and 'big' are at hand, whereas color adjectives are more often mentioned superfluously because they have more absolute meaning and do not need immediate comparisons. This possiblity is consistent with the model we have specified, and is just one possible reason for a production asymmetry which listeners are responding to rationally in their inferences. Another possibility is that the gradable, relative nature of size adjectives should be encoded in the pragmatic learner part of the model: a learner might need a comparison point to tell whether a novel object is small or big, but not red or purple, and thus avoid choosing a unique (shaped) object when size is specified but be willing to choose a unique object when color is specified. This possibility would require more fundamental changes to the model. Here, we make the conservative choice to encode the color-size asymmetry in the broad rationality parameter, though changing the pragmatic learner's decision process is an intriguing possibility for future work.

Overall, we found that people can use contrastive inferences from description to map an unknown word to an unknown object. This inference is captured by an extension of the Rational Speech Act model using a pragmatic learner, who is simultaneously making inferences over possible referents and possible lexicons. This model can also capture people's tendency to make stronger contrastive inferences from color description than size description through differences in the rationality parameter, though the origin of these differences cannot be pinned down with this experiment alone. Our experiment and model results suggest that people can resolve a request like "Give me the small dax" by reasoning that the speaker must have been making a useful distinction by mentioning size, and therefore looking for multiple similar objects that differ in size and choosing the smaller one. Immediately available objects are not the only ones worth making a distinction from, however. Next, we turn to another salient set of objects a speaker might want to set a referent apart from: the referent's category.