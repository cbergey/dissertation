Children learn a tremendous amount about the structure of the world around them in just a few short years, from the rules that govern the movement of physical objects to the hierarchical structure of natural categories and even the relational structures among social and cultural groups [@baillargeon1994; @rogers2004; @legare2016]. Where does the information driving this rapid acquisition come from? Undoubtedly, a sizeable portion comes from direct experience observing and interacting with the world [@sloutsky2004; @stahl2015]. But another important source of information comes from the language people use to talk about the world [@landauer1997; @rhodes2012]. How similar is the information from children's direct experience to the information available in the language children hear? 

Two lines of work suggest that they may be surprisingly similar. One compelling area of work is the comparison of semantic structures learned by congenitally blind children to those of their sighted peers. In several domains that would seem at first blush to rely heavily on visual information, such as verbs of visual perception (e.g., *look*, *see*), blind children and adults make semantic similarity judgments that mirror their sighted peers [@landau2009; @bedny2019]. A second line of evidence supporting the similarity of information in perception and language is the broad success of statistical models trained on language alone in approximating human judgments across a variety of domains [@landauer1997; @mikolov2013]. Even more compellingly, models trained on both language usage and perceptual features for some words can infer the perceptual features of linguistically related words entirely from the covariation of language and perception [@johns2012]. 
<!-- @devlin2018 -->

Still, there is reason to believe that some semantic features may be harder to learn from language than these data suggest. This is because we rarely use language merely to provide running commentary on the world around us; instead, we use language to talk about things that diverge from our expectations or those of our conversational partner [@grice1975logic]. People tend to avoid being over- or under-informative when they speak. In particular, when referring to objects, people are informative with respect to both the referential context and the typical features of the referent [@westerbeek2015; @rubio-fernandez2016]. People tend to refer to an object that is typical of its category with a bare noun (e.g., calling an orange carrot "a carrot"), but often specify when an object has an atypical feature (e.g, "a purple carrot"). Given these communicative pressures, naturalistic language statistics may provide surprisingly little evidence about what is typical [@willits2008]. 

```{r utt_table, results="asis", tab.env = "table"}
tab <- tibble(utterance = c("especially with wooden shoes.",
                            "you like red onions?", 
                            "the garbage is dirty."),
              pair = c("wooden-shoe", "red-onion", "dirty-garbage"),
              `rating 1` = c(2, 3, 7),
              `rating 2` = c(2, 5, 7),
              `rating 3` = c(3, 3, 5),
              `rating 4` = c(2, 4, 7),
              `mean typicality` = c(2.25, 3.75, 6.5)) %>% 
  xtable(display = c("s", "s", "s", "d", "d", "d", "d", "f"),
         caption = "Sample typicality ratings from 4 human coders for three adjective-noun pairs drawn from the corpus.",
         label = "tab:utt_table")

#print(tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
#      floating.environment = "table*",
#      include.rownames = FALSE)
```

<!-- This is because people rarely use language merely to provide running comentary on the world around them. Instead, we use language to talk about things we find interesting--things that are surprising, exciting, or otherwise divergent from our conversational partners' expectations [@grice1975].  -->
<!--When faced with an object that is typical of its kind, speakers overwhelmingly refer to that object with a bare noun- e.g., calling a banana that is yellow "a banana." But, when faced with an object that is atypical of its kind, speakers consistently produce description as well- e.g., calling a banana that is blue "a blue banana." This allows speakers to be as informative as needed, providing no description when it should be assumed (yellow banana) and providing information when it is unexpected (blue banana). These two empirical, lab-based phenomena demonstrate communicative pressures which, at scale, could mean naturalistic language is structured to contain much more description of what is atypical.-->

If parents speak to children in this minimally informative way, children may be faced with input that emphasizes atypicality in relation to world knowledge they do not yet have. For things like carrots---which children learn about both from perception and from language---this issue may be resolved by integrating both sources of information. Likely almost all of the carrots children see are orange, and hearing an atypical exemplar noted as a "purple carrot" may make little difference in their inferences about the category of carrots more broadly. But for things to which they lack perceptual access---such as rare objects, unfamiliar social groups, or inaccessible features like the roundness of the Earth---much of what they learn must come from language [@harris2006]. If language predominantly notes atypical features rather than typical ones, children may overrepresent atypical features as they learn the way things in the world tend to be.

On the other hand, parents may speak to children differently from the way they speak to other adults. Parents' speech may reflect typical features of the world more veridically, or even emphasize typical features in order to teach children about the world. Parents alter their speech to children along a number of structural dimensions, using simpler syntax and more reduplications [@snow1972]. Their use of description may reflect similar alignment to children's abilities by emphasizing typical feature information children are still learning.

We examine the typicality of adjectives in a large, diverse corpus of parent-child interactions recorded in children's homes to ask whether parents talking to their children tend to use adjectives predominantly to mark atypical features. We find that they do: Parents overwhelmingly choose to mention atypical rather than typical features. We also find that parents use adjectives differently over the course of children's development, noting typical features more often to younger children. We then ask whether the co-occurrence structure of language nonetheless captures typicality information by training vector space models on child-directed speech. We find that relatively little typical feature information is represented in these semantic spaces. 

# Adjective typicality

In order to determine whether parents use adjectives mostly to mark atypical features of categories, we analyzed caregiver speech from a large corpus of parent-child interactions. We extracted a subset of adjective-noun combinations that co-occurred, and asked a sample of Amazon Mechanical Turkers to judge how typical the property described by each adjective was for the noun it modified. We then examined both the broad features of this typicality distribution and the way it changes over development. Our theoretical hypotheses, statistical models, sample size, and exclusion criteria were pre-registered on the Open Science Framework (\url{https://osf.io/ypdzv/}).

## Corpus

We used data from the Language Development Project, a large-scale, longitudinal corpus of parent-child interactions recorded in children's homes. Families were recruited to be representative of the Chicagoland area in both socio-economic and racial composition [@goldin-meadow2014]. Recordings were taken in the home every 4 months from when the child was 14 months old until they were 58 months old, resulting in 12 timepoints. Each recording was of a 90-minute session in which parents and children were free to behave and interact as they liked.

Our sample consisted of 64 typically-developing children and their caregivers with data from at least 4 timepoints (*mean* = 11.3 timepoints). Together, this resulted in a total of 641,402 distinct parent utterances.  

## Stimulus Selection

From these utterances, we extracted all of the nouns (using human-coded part of speech tags) resulting in a set of 8,150 total nouns. Because of our interest in change over development, we considered only nouns that appeared at least once every 3 sessions (i.e. at least once per developmental year). This yielded a set of 1,829 potential target nouns used over 198,014 distinct utterances.

We selected from the corpus all 35,761 distinct utterances containing any of these nouns and any word tagged as an adjective. We considered for analysis all adjective-noun pairs that occurred in any utterance (e.g., utterances with one noun and three adjectives were coded as three pairs) for a total of 18,050 distinct pairs. This set contained a number of high-frequency idiomatic pairs whose typicality was difficult to classify (e.g., "good"--"job"; "little"--"bit"). To resolve this issue, we used human judgments of words' concreteness to identify and exclude candidate idioms [@brysbaert2014]. We retained for analysis only pairs in which both the adjective and noun were in the top 25% of the concreteness ratings (e.g., "dirty" -- "dish"; "green" -- "fish") restricting our set to 2,477.  Finally, human coders in the lab judged whether each pair was "incoherent or unrelated" and we excluded a final 576 pairs from the sample (e.g., incoherent pairs such as "flat" -- "honey").

Thus, our final sample included 1,901 unique adjective-noun pairs drawn from 3,749 distinct utterances. The pairs were combinations of 637 distinct concrete nouns and 111 distinct concrete adjectives. We compiled these pairs and collected human judgments on Amazon Mechanical Turk for each pair, as described below. Table \ref{tab:utt_table} contains example utterances from the final set and typicality judgments from our human raters.

## Participants

Each participant rated 20 adjective-noun pairs, and each pair was rated by four participants; we used [Dallinger](http://docs.dallinger.io/en/latest/), a tool for automating complex recruitment in online studies, to balance coding of the pairs. Overall, we recruited 444 participants to rate our final sample of adjective–noun pairs. After exclusions using an attention check that asked participants to simply choose a specific number on the scale, we retained 8,580 judgments, with each adjective–noun pair retaining at least two judgments.

## Design and Procedure

To evaluate the typicality of the adjective–noun pairs that appeared in parents' speech, we asked participants on Amazon Mechanical Turk to rate each pair. Participants were presented with a question of the form “How common is it for a cow to be a brown cow?” and asked to provide a rating on a seven-point scale: (1) never, (2) rarely, (3) sometimes, (4) about half the time, (5) often, (6) almost always, (7) always.

```{r read_tokens}
token_data <- read_csv(here("data/ch1/clean_token_data.csv"))

coca_data <- read_csv(here("data/ch1/adult_tokens_withn.csv")) %>%
  filter(!is.na(n))

kid_production <- read_csv(here("data/ch1/kid_production.csv")) %>%
  mutate(age = 10 + 4 * session) %>%
  select(adj, noun, n, age) %>%
  distinct() %>%
  left_join(select(token_data, adj, noun, x1:x5) %>% distinct(), by = c("adj", "noun")) %>%
  pivot_longer(x1:x5, names_to = "rater", values_to = "score")

kid_total <- kid_production %>%
  distinct(adj, noun) %>%
  count() %>%
  pull()

kid_frac <- kid_production %>%
  group_by(adj, noun) %>%
  summarise(score = mean(score, na.rm = TRUE)) %>%
  filter(!is.na(score)) %>%
  ungroup() %>%
  count() %>%
  pull()

type_data_by_session <- token_data %>%
  group_by(adj, noun, session, age, noun_conc, noun_freq, 
           article, x1, x2, x3, x4, x5, mean_typ) %>%
  summarise(n = n())
```
```{r models}
# old file without bert probs
#model_judgments <- read_csv(here("data/all_judgments.csv"))
model_judgments <- read_csv(here("data/ch1/all_models_probs.csv"))

tidy_turk_counts <- type_data_by_session %>%
    # select(-noun_conc, -noun_freq) %>%
    pivot_longer(cols = x1:x5, names_to = "rater", values_to = "score") %>%
  ungroup() %>%
    mutate(centered_score = score - 4,
           article = case_when(is.na(article) ~ "",
                               article %in% c("a","an") ~ "a/an",
                               T ~ "the")) %>%
  left_join(model_judgments %>% select(-turker_judgment), by = c("adj","noun"))


mean_type <- lmer(centered_score ~ 1 + (1|noun),
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_type_byage <- tidy_turk_counts %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_score ~ 1 + (1|noun),
                     data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()

mean_token_byage <- tidy_turk_counts %>%
  group_by(age) %>%
  nest() %>%
  mutate(effect = map(data, ~lmer(centered_score ~ 1 + (1|noun),
                                  weights = n, data = .) %>%
                       tidy() %>%
                       filter(effect == "fixed"))) %>%
  select(-data) %>%
  unnest(cols = c(effect)) %>%
  ungroup()


mean_token <- lmer(centered_score ~ 1 + (1|noun), weights = n,
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


token_weight <- lmer(centered_score ~ log(age) + (1|noun),
                     weights = n,
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


type_weight <- lmer(centered_score ~ log(age) + (1|noun),
                     data = tidy_turk_counts) %>%
                     tidy() %>%
                     filter(effect == "fixed")


mean_coca_token <- coca_data %>%
  filter(type == "spok") %>%
  pivot_longer(cols = c(x1:x5), names_to = "rater", values_to = "score") %>%
  mutate(centered_score = score - 4) %>%
  lmer(centered_score ~ 1 + (1|noun), weights = n, data  = .) %>%
                     tidy() %>%
                     filter(effect == "fixed")

mean_kid_token <- kid_production %>%
  mutate(centered_score = score - 4) %>%
  lmer(centered_score ~ 1 + (1|noun), weights = n, data  = .) %>%
                     tidy() %>%
                     filter(effect == "fixed")


```
```{r result-effects}
token_estimate <- mean_token %>% pull(estimate)
token_statistic <- mean_token %>% pull(statistic)
token_p <- mean_token %>% pull(p.value) %>% printp()

token_age_estimate <- mean_token_byage %>% slice(1) %>% pull(estimate)
token_age_statistic <- mean_token_byage %>% slice(1) %>% pull(statistic)
token_age_p <- mean_token_byage %>% slice(1) %>% pull(p.value) %>% printp()

coca_token_estimate <- mean_coca_token %>% pull(estimate)
coca_token_statistic <- mean_token %>% pull(statistic)
coca_token_p <- mean_token %>% pull(p.value) %>% printp()

token_development_estimate <-token_weight %>% filter(term == "log(age)") %>% 
  pull(estimate)
token_development_statistic <-token_weight %>% filter(term == "log(age)") %>% 
  pull(statistic)
token_development_p <-token_weight %>% filter(term == "log(age)") %>% 
  pull(p.value) %>% printp()

kid_token_estimate <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(estimate)
kid_token_statistic <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(statistic)
kid_token_p <- mean_kid_token %>% filter(term == "(Intercept)") %>%
  pull(p.value) %>% printp()
```

## Results

The human typicality ratings were combined with usage data from our corpus analysis to let us determine the extent to which parents use language to describe typical and atypical features. In our analyses, we token-weighted these judgments, giving higher weight to pairs that occurred more frequently in children's inputs. However, results are qualitatively identical and all significant effects remain significant without these re-weightings.

If caregivers speak informatively to convey what is atypical or surprising in relation to their own sophisticated world knowledge, we should see that caregiver description is dominated by adjectives that are sometimes or rarely true of the noun they modify. If instead child-directed speech privileges redundant information, perhaps to align to young children's limited world knowledge, caregiver description should yield a distinct distribution dominated by highly typical modifiers. As predicted in our pre-registration, we find that parents' description predominantly focuses on features that are atypical (Figure \ref{fig:distribution-plot}).

```{r distribution-plot, fig.align = "center", fig.width=3.4, fig.height = 5.4, fig.cap = "Density plots showing usage at each timepoint based on the typicality of the adjective-noun pair.", fig.pos="tb"}

token_data %>%
  mutate(typicality=mean_typ) %>%
  mutate(age = (4*session + 10)) %>%
  group_by(session) %>%
  mutate(age = min(age)) %>%
  ggplot(aes(x = typicality, y=age, group=age, fill=age)) +
  geom_density_ridges2() +
  ylab("Child Age (months)") +
  xlab("More Atypical                   More Typical \n Typicality of adjective-noun pairs") +
  geom_vline(xintercept = 4, size=1, linetype="solid")+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme_few() +
  theme(#panel.grid = element_line(color="lightgrey",size=0.5), 
    axis.line = element_line(colour = "black"),
    axis.ticks = element_line(),
    axis.text.x = element_text(size=11, angle=28, hjust=1),
    axis.text.y = element_text(size=11),
    legend.position = "none") +
  scale_x_continuous(minor_breaks = seq(1 , 7, 1), breaks = seq(1, 7, 1), labels = c('never', 'rarely', 'sometimes', 'about half', 'often', 'almost always', 'always')) +
  scale_y_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))
```

To confirm this effect statistically, we centered the ratings (i.e. "about half" was coded as 0), and then predicted the rating on each trial with a mixed effect model with only an intercept and a random effect of noun (\texttt{typicality $\sim$ 1 + (1|noun)}). The intercept was reliably negative, indicating that adjectives tend to refer to atypical features of objects ($\beta =$ `r token_estimate`, $t =$ `r token_statistic`, $p$ `r token_p`). We then re-estimated these models separately for each age in the corpus, and found a reliably negative intercept for every age group (smallest effect $\beta_{14} =$ `r token_age_estimate`, $t =$ `r token_age_statistic`, $p =$ `r token_age_p`).  These data suggest that even when talking with very young children, caregiver speech is structured according to the kind of communicative pressures observed in adult-adult conversation in the lab.

<!-- Examining usage data as a function of typicality (see Figure \ref{fig:distribution_plot}), we see evidence of a positive skew (0.65). Data from every time point from 14-58 months seems to show a similar pattern (skews 0.23 - 0.82). These skews provide further evidence that the the bulk of caregiver language reflects lower-typicality adjective-noun pairs. -->

For comparison, we performed the same analyses but with typicality judgments weighted not by the frequency of each adjective-noun pair's occurrence in the Language Development Project, but instead by their frequency of occurrence in the Corpus of Contemporary American English [COCA; @davies2008]. While this estimate of adult usage is imperfect—the adjective-nouns pairs produced by parents in our corpus may not be a representative sample of adjectives and nouns spoken by the adults in COCA—it provides a first approximation to adult usage. When we fit the same mixed-effects model to the data, we found that the intercept was reliably negative, indicating that adult-to-adult speech is likely also biased toward description of atypical features ($\beta =$ `r coca_token_estimate`, $t =$ `r coca_token_statistic`, $p$ `r coca_token_p`). We propose to carry out a fuller analysis of adult-adult speech, using the same method we used for the LDP, for the full dissertation.

```{r compute_prototypicals}
prototypical_ratings <- type_data_by_session %>%
  mutate(typical = mean_typ >= 5)

#look for prototypicals
#  defined as anything 5 or higher, "somewhat typical" to "extremely  typical"
prototypicals <- prototypical_ratings %>%
  group_by(age, typical) %>%
  summarise(weighted_sum = sum(n), sum = n()) %>%
  group_by(age, typical) %>%
  summarise(sum = sum(sum), weighted_sum = sum(weighted_sum)) %>%
  pivot_longer(cols = c(sum, weighted_sum), names_to = "measure", values_to = "sum") %>%
  group_by(age, measure) %>%
  mutate(prop = sum / sum(sum)) %>%
  filter(typical)


typical_type_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = prototypical_ratings, 
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")

typical_token_lmer <- glmer(typical ~ log(age) + (1|noun), 
                      data = prototypical_ratings, 
                      weights = n,
     family = "binomial") %>% 
  tidy() %>%
  filter(effect == "fixed")
```

```{r typical-effects}
typical_effect <- typical_token_lmer %>% filter(term == "log(age)") %>%
  pull(estimate)
typical_statistic <- typical_token_lmer %>% filter(term == "log(age)") %>% 
  pull(statistic)
typical_p <- typical_token_lmer %>% filter(term == "log(age)") %>% 
  pull(p.value) %>% printp()
```

Returning to caregiver speech, while descriptions at every age tended to point out atypical features (as in adult-to-adult speech), this effect changed in strength over development. As predicted, an age effect added to the previous model was reliably negative, indicating that parents of older children are relatively more likely to focus on atypical features  ($\beta =$ `r token_development_estimate`, $t =$ `r token_development_statistic`, $p =$ `r token_development_p`). In line with the idea that caregivers adapt their speech to their children's knowledge, it seems that caregivers are more likely to provide description of typical features for their young children, compared with older children. As a second test of this idea, we defined adjectives as highly typical if Turkers judged them to be 'often', 'almost always', or 'always' true. We predicted whether each judgment was highly typical from a mixed-effects logistic regression with a fixed effect of age (log-scaled) and a random effect of noun. Age was a highly reliable predictor ($\beta =$ `r typical_effect`, 
$t =$ `r typical_statistic`, $p =$ `r typical_p`). While children at all ages hear more talk about what is atypically true (Figure  \ref{fig:distribution-plot}), younger children hear relatively more talk about what is typically true than older children do (Figure \ref{fig:prototypical-plot}).

```{r prototypical-plot, fig.align = "center", fig.cap = "Proportion of caregiver description that is about typically-true features, as a function of age."}

prototypicals %>% 
  filter(measure == "weighted_sum") %>% 
  ggplot(aes(x = age,y = prop, colour = age)) +
  geom_smooth(method = "glm", formula = y~x,
                      method.args = list(family = gaussian(link = 'log')),color = "black") +
  geom_point(aes(fill=age), colour="black",pch=21, size=5) +
  ylab("Proportion of modifiers rated as \n typical of modified noun") +
  xlab("Child's Age (months)") +
  scale_x_continuous(minor_breaks = seq(14, 58, 4), breaks = seq(14, 58, 4))+
  scale_fill_gradient(low="cornsilk", high=muted("red")) +
  theme(#axis.line = element_line(colour = "black"),
        #axis.ticks = element_line(),
        #axis.text = element_text(size=14),
        #panel.grid = element_line(color="lightgrey",size=0.5),
        #axis.text.x = element_text(size=10, angle=15),
        legend.position = "none",
        aspect.ratio = 1/1.62) 
```

## Discussion

In sum, we find robust evidence that language is used to discuss atypical, rather than typical, features of the world. Description in caregiver speech seems to largely mirror the usage patterns that we observed in adult-to-adult speech, suggesting that these patterns arise from general communicative pressures. Interestingly, the descriptions children hear change over development, becoming increasingly focused on atypical features. The higher prevalence of typical descriptors in early development may help young learners learn what is typical; however, even at the earliest point we measured, the bulk of language input describes atypical features. 

<!-- their pattern of description is likely highly dependent on their caregiver's utterances.... Thus, it is possible that some of the children's use of atypical description is prompted by a parent-led discourse. -->
<!-- Across adult, parent, and child language corpora, we find robust evidence that language use systematically overerpresents atypical features.  -->

This usage pattern aligns with the idea that language is used informatively in relation to background knowledge about the world. It may pose a problem, however, for young language learners with still-developing world knowledge. If language does not transparently convey the typical features of objects, and instead (perhaps misleadingly) notes the atypical ones, how might children come to learn what objects are typically like? One possibility is that information about typical features is captured in more complex regularities across many utterances. If this is true, language may still be an important source of information about typicality as children may be able to extract more accurate typicality information by tracking second-order co-occurrence.

# Extracting Typicality from Language Structure

Much information can be gleaned from language that does not seem available at first glance. From language alone, simple distributional learning models can recover enough information to perform comparably to non-native college applicants on the Test of English as a Foreign Language [@landauer1997]. Recently, @lewis2019 demonstrated that even nuanced feature information may be learnable through distributional semantics alone, without any complex inferential machinery. We take a similar approach to ask whether a distributional semantics model trained on the language children hear can capture typical feature information. 

```{r word2vec cors}
tidy_turk_counts <- tidy_turk_counts %>%
  filter(!(adj == noun)) %>%
  filter(!(adj == "orange")) %>%
  filter(!(noun == "orange"))

tidy_turk_counts_for_plots <- tidy_turk_counts %>%
  distinct(noun,adj,mean_typ,ldp_similarity,wiki_similarity, bert_p, bert_gen)

word2vec_cor_data <- tidy_turk_counts %>%
  group_by(adj, noun, wiki_similarity, ldp_similarity, bert_p, bert_gen) %>%
  summarise(typicality = mean(score, na.rm = T), n = sum(n)) 

ldp_cor <- wtd.cor(word2vec_cor_data %>% pull(typicality),
                    word2vec_cor_data %>% pull(ldp_similarity)) %>%
  as_tibble()

wiki_cor <- wtd.cor(word2vec_cor_data %>% pull(typicality),
                    word2vec_cor_data %>% pull(wiki_similarity)) %>%
  as_tibble()

bert_p_cor <- wtd.cor(word2vec_cor_data %>% pull(typicality),
                    word2vec_cor_data %>% pull(bert_p)) %>%
  as_tibble()

bert_gen_cor <- wtd.cor(word2vec_cor_data %>% pull(typicality),
                    word2vec_cor_data %>% pull(bert_gen)) %>%
  as_tibble()


models_cor <- wtd.cor(word2vec_cor_data %>% pull(ldp_similarity),
                    word2vec_cor_data %>% pull(wiki_similarity)) %>%
  as_tibble()

bertp_wikiw2v_cor <- wtd.cor(word2vec_cor_data %>% pull(bert_p),
                    word2vec_cor_data %>% pull(wiki_similarity)) %>%
  as_tibble()

bertgen_wikiw2v_cor <- wtd.cor(word2vec_cor_data %>% pull(bert_gen),
                    word2vec_cor_data %>% pull(wiki_similarity)) %>%
  as_tibble()

ldp_cor_estimate <- ldp_cor %>% pull(correlation)
ldp_cor_p <- ldp_cor %>% pull(p.value) %>% printp()


wiki_cor_estimate <- wiki_cor %>% pull(correlation)
wiki_cor_p <- wiki_cor %>% pull(p.value) %>% printp()

bert_p_cor_estimate <- bert_p_cor %>% pull(correlation)
bert_p_cor_p <- bert_p_cor %>% pull(p.value) %>% printp()
bert_gen_cor_estimate <- bert_gen_cor %>% pull(correlation)
bert_gen_cor_p <- bert_gen_cor %>% pull(p.value) %>% printp()

w2v_models_cor_estimate <- models_cor %>% pull(correlation)
w2v_models_cor_p <- models_cor %>% pull(p.value) %>% printp()
w2v_bertp_cor_estimate <- bertp_wikiw2v_cor %>% pull(correlation)
w2v_bertp_cor_p <- bertp_wikiw2v_cor %>% pull(p.value) %>% printp()
w2v_bertgen_cor_estimate <- bertgen_wikiw2v_cor %>% pull(correlation)
w2v_bertgen_cor_p <- bertgen_wikiw2v_cor %>% pull(p.value) %>% printp()

```

```{r, eval = FALSE}
split_half <- tidy_turk_counts %>%
  mutate(rater = as.numeric(gsub("x", "", rater))) %>%
  filter(rater %in% c(1, 2)) %>%
  group_by(rater, noun, adj) %>%
  summarise(score = mean(score, na.rm = T)) %>%
  pivot_wider(names_from = rater, values_from = score)

half_model <- lmer(`1` ~ `2` + (1|adj) + (1|noun), data = split_half)

split_half %>%
  ungroup() %>%
  mutate(prediction = predict(half_model)) %>%
  summarise(cor = cor(`1`, prediction))

data <- tidy_turk_counts_for_plots %>% 
       left_join(tidy_turk_counts %>% filter(rater == "x1"))

cor(data$score, data$wiki_similarity, use = "pairwise")

wiki_model <- lm(score ~ wiki_similarity ,
     data = data) 

tidy_turk_counts_for_plots %>%
  filter(!is.na(wiki_similarity)) %>%
  mutate(prediction = predict(wiki_model)) %>%
  summarise(cor = cor(prediction, turker_judgment))

```

```{r word2vec-pairs}
pairs <- tidy_turk_counts %>%
  group_by(noun, adj) %>%
  summarise(typicality = mean(score, na.rm = TRUE)) %>%
  group_by(noun) %>%
  mutate(max_typ = max(typicality),
         min_typ = min(typicality)) %>%
  distinct(noun, min_typ, max_typ) %>%
  filter(min_typ != max_typ, max_typ >= 5, min_typ <= 3) 

high_low_pairs <- tidy_turk_counts_for_plots %>%
  filter(noun %in% pairs$noun) %>%
  left_join(pairs, by = c("noun")) %>%
  filter(mean_typ == min_typ | mean_typ == max_typ) %>%
  select(adj, noun, mean_typ) %>%
  group_by(noun) %>%
  arrange(noun, desc(mean_typ)) %>%
  slice(1, n()) %>%
  mutate(typicality = c("high", "low"),
         typicality = factor(typicality, levels = c("low", "high"))) %>%
  left_join(tidy_turk_counts_for_plots)

#write_csv(high_low_pairs, here("data/high_low_pairs.csv"))

correct_orders <- high_low_pairs %>%
  pivot_longer(cols = c(ldp_similarity, wiki_similarity, bert_gen, bert_p), 
               names_to = "measure", values_to = "similarity") %>%
  select(-adj, -mean_typ) %>%
  pivot_wider(names_from = "typicality", values_from = "similarity") %>%
  mutate(correct = high - low > 0) %>%
  filter(!is.na(correct)) %>%
  group_by(measure) %>%
  summarise(correct = sum(correct), total = n())

ldp_correct <- correct_orders %>%
  filter(measure == "ldp_similarity") %>%
  pull(correct)

wiki_correct <- correct_orders %>%
  filter(measure == "wiki_similarity") %>%
  pull(correct)

bertp_correct <- correct_orders %>%
  filter(measure == "bert_p") %>%
  pull(correct)

bertgen_correct <- correct_orders %>%
  filter(measure == "bert_gen") %>%
  pull(correct)

pairs_total <- correct_orders %>% pull(total) %>% first()

ldp_binom <- binom.test(ldp_correct, pairs_total)$p.value %>%
  printp()
wiki_binom <- binom.test(wiki_correct, pairs_total)$p.value %>%
  printp()
bertp_binom <- binom.test(bertp_correct, pairs_total)$p.value %>%
  printp()
bertgen_binom <- binom.test(bertgen_correct, pairs_total)$p.value %>%
  printp()
```

```{r}

# The black dotted line shows average human typicality ratings (scaled) for these items. The blue line shows how well our models do at capturing this trend, with grey lines representing individual pairs

halves_data <- high_low_pairs %>%
  pivot_longer(cols = c(mean_typ, wiki_similarity, ldp_similarity, bert_gen, bert_p),
              names_to = "measure", values_to = "score") %>%
  mutate(measure = factor(measure, 
                          levels = c("mean_typ", "ldp_similarity", 
                                     "wiki_similarity", "bert_p", "bert_gen"), 
                          labels = c("Human", "LDP word2vec", 
                                     "Wiki word2vec", "BERT prenominal", "BERT predicate"))) %>%
  mutate(score = if_else(measure=="Human", score/7, score)) %>%
  filter(! noun == "orange") %>%
    filter(! adj == "orange") 

human <- halves_data %>%
  filter(measure =="Human") %>%
  group_by(typicality) %>% 
  summarise(mean=mean(score))

means <- halves_data %>%
    group_by(measure, typicality) %>% 
    summarise(mean=mean(score, na.rm=T)) %>%
    filter(measure != "Human")


grob <- grobTree(textGrob("Human ratings \n (scaled)", x=0.97,  y=0.85, hjust=1,
  gp=gpar(col="Black", fontsize=9, fontface="italic")))
  
grob2 <- grobTree(textGrob("Model average", x=.96,  y=0.33, hjust=1,
  gp=gpar(col="steelblue", fontsize=9, fontface="italic")))

typicality_axis <- c("Low Typicality", "High Typicality")

```

## Method

To test this possibility, we trained word2vec--a distributional semantics model--on the same corpus of child-directed speech used in our first set of analyses. Word2vec is a neural network model that learns to predict words from the contexts in which they appear. This leads word2vec to encode words that appear in similar contexts as similar to one another [@firth1957].

We used the continuous-bag-of-words (CBOW) implementation of word2vec in the `gensim` package [@rehurek2010]. We trained the model using a surrounding context of 5 words on either side of the target word and 100 dimensions (weights in the hidden layer) to represent each word. After training, we extracted the hidden layer representation of each word in the model's vocabulary--these are the vectors used to represent these words. 

If the model captures information about the typical features of objects, we should see that the model's noun-adjective word pair similarities are correlated with the typicality ratings we elicited from human raters. For a second comparison, we also used an off-the-shelf implementation of word2vec trained on Wikipedia [@mikolov2018]. While the Language Development Project corpus likely underestimates the amount of structure in children's linguistic input, Wikipedia likely overestimates it.

While word2vec straightforwardly represents what can be learned about word similarity by associating words with similar contexts, it does not represent the cutting edge of language modeling. Perhaps a more sophisticated model, trained on a larger corpus, would represent these typicalities better. To test this, we asked how BERT [@devlin2018], a masked language model trained on English Wikipedia and BookCorpus, represents typicality. BERT does not directly provide similarity metrics between words, so to ask this, we must embed the pairs in sentential contexts. Since the placement of the adjective in the sentence may affect BERT's judgments, we used both a prenominal adjective sentence frame, which intuitively may express more atypical information, and a predicate adjective sentence frame, which intuitively may express more typical information. We gave BERT sentences of the form "I saw the ____ apple" (prenominal frame) and "The apple is ____" (predicate frame), and asked it the probability of different adjectives filling the empty slot. Because BERT has more complex training objectives and is trained on a much larger corpus than word2vec, results from BERT likely do not straightforwardly represent the information available to children in language. However, results from BERT can indicate the challenges language models face in representing world knowledge when the language people use emphasizes remarkable rather than typical situations.

## Results

We find that similarities in the model trained on the Language Development Project corpus have near zero correlation with human adjective–noun typicality ratings ($r =$ `r ldp_cor_estimate`, $p =$ `r ldp_cor_p`). However, our model does capture other meaningful information about the structure of language, such as similarity. Comparing with pre-existing large-scale human similarity judgements for word pairs, our model shows significant correlations (correlation with wordsim353 similarities of noun pairs, 0.28; correlation with simlex similarities of noun, adjective, and verb pairs, 0.16). This suggests that statistical patterns in child-directed speech are likely insufficient to encode information about the typical features of objects, despite encoding at least some information about word meaning more broadly. 

However, the corpus on which we trained this model was small; perhaps our model did not get enough language to draw out the patterns that would reflect the typical features of objects. To test this possibility, we asked whether word vectors trained on a much larger corpus—English Wikipedia—correlate with typicality ratings. This model's similarities were significantly correlated with human judgments, although the strength of the correlation was still fairly weak ($r =$ `r wiki_cor_estimate`, $p$ `r wiki_cor_p`). How does an even larger and more sophisticated language model, BERT, fare? Like Wikipedia-trained word2vec, BERT's probabilities were significantly correlated with human judgments, though weakly so (prenominal adjective: $r =$ `r bert_p_cor_estimate`, $p$ `r bert_p_cor_p`; predicate adjective: $r =$ `r bert_gen_cor_estimate`, $p$ `r bert_gen_cor_p`). 

One possible confound in these analyses is that the similarity judgments produced by our models reflect many dimensions of similarity, but our human judgments reflect only typicality. To accommodate this, we performed a second analysis in which we considered only the subset of `r pairs_total` nouns that had both a typical (rated as at least "often") and an atypical (rated as at most "sometimes") adjective. We then asked whether the models rated the typical adjective as more similar to the noun it modified than the atypical adjective. The LDP model correctly classified `r ldp_correct` out of `r pairs_total` (`r ldp_correct / pairs_total`), which was not better than chance ($p =$ `r ldp_binom`). The Wikipedia-trained word2vec model correctly classified `r wiki_correct` out of `r pairs_total` (`r wiki_correct / pairs_total`), which was better than chance according to a binomial test, but still fairly poor performance ($p =$ `r wiki_binom`). BERT correctly classified `r bertp_correct` out of `r pairs_total` (`r bertp_correct / pairs_total`) in the prenominal sentence frame, which is not significantly better than chance ($p =$ `r bertp_binom`) and `r bertgen_correct` (`r bertgen_correct / pairs_total`) in the predicate sentence frame, which is significantly better than chance ($p =$ `r bertgen_binom`). Both sets of BERT ratings are directionally less accurate than those of Wikipedia-trained word2vec: using a more advanced model did not improve performance on this task. Figure \ref{fig:halfs} shows the ratings from Turkers and the models for the `r pairs_total` nouns and their typical and atypical adjectives. Table \ref{tab:pairs_tab} gives the six cases in which word2vec similarities are worst at predicting human typicality judgments, judging the low-typicality adjective to be *more* similar to the noun than the high-typicality adjective.

```{r halfs, cache=F, fig.width=3.4, fig.height = 5.4, fig.align = "center", fig.cap = 'Plots of word2vec and BERT noun-adjective similarities for nouns for which there was at least one atypical adjective (rated at most "sometimes"), and at least one typical adjective (rated at least "often").'}

halves_data %>%
  filter(measure != "Human") %>%
  ggplot(aes(x = typicality, y = score, group = noun)) +
  facet_grid(measure ~ .) +
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  geom_line(data=human, aes(x=typicality, y=mean, group=NA), color="black", linetype="dashed") +
  geom_point(data=human, aes(x=typicality, y=mean, group=NA), color="black") +
  geom_line(data=means, aes(x=typicality, y=mean, group=NA), color="steelblue", size=.8) +
  geom_point(data=means, aes(x=typicality, y=mean, group=NA), color="steelblue") +
  labs(x = "Binned Human-Rated Typicality", y = "Cosine Similarity") +
  theme_few() +
  coord_cartesian(ylim=c(-0.17,1)) +
  annotation_custom(grob) +
  annotation_custom(grob2) +
  scale_x_discrete(labels= typicality_axis, expand = c(.2, .2))

bert_plot <- halves_data %>%
  filter(measure %in% c("BERT prenominal", "BERT predicate")) %>%
  mutate(measure = factor(measure, 
                          levels = c("BERT prenominal", "BERT predicate"))) %>%
  ggplot(aes(x = typicality, y = score, group = noun)) +
  facet_wrap(~measure) +
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  geom_line(data=human, aes(x=typicality, y=mean, group=NA), color="black", linetype="dashed") +
  geom_point(data=human, aes(x=typicality, y=mean, group=NA), color="black") +
  geom_line(data=means %>% 
              filter(measure %in% c("BERT prenominal", "BERT predicate")), 
            aes(x=typicality, y=mean, group=NA), color="steelblue", size=.8) +
  geom_point(data=means %>%
               filter(measure %in% c("BERT prenominal", "BERT predicate")), 
             aes(x=typicality, y=mean, group=NA), color="steelblue") +
  labs(x = "Binned Human-Rated Typicality", y = "Probability") +
  theme_few() +
  coord_cartesian(ylim=c(-0.17,1)) +
  annotation_custom(grob) +
  annotation_custom(grob2) +
  scale_x_discrete(labels= typicality_axis, expand = c(.2, .2)) +
  theme(
    strip.background =element_rect(fill="transparent"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    panel.grid.major = element_blank(), # get rid of major grid
    panel.grid.minor = element_blank(), # get rid of minor grid
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent"), # get rid of legend panel bg
    legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
    axis.line = element_line(colour = "black") # adding a black line for x and y axis
)

ggsave(bert_plot, filename = "bert_plot.png",  width = 6, height = 4, units = "in", dpi = 300, bg = "transparent")
```

```{r pairs_tab, results="asis", tab.env = "table"}
pair_tab <- high_low_pairs %>%
    filter(! noun == "orange") %>%
    filter(! adj == "orange") %>%
  group_by(noun) %>%
  summarise(diff = first(wiki_similarity) - last(wiki_similarity)) %>%
  filter(diff < 0) %>%
  left_join(high_low_pairs %>% select(noun, adj)) %>%
  group_by(noun) %>%
  mutate(typicality = c("high", "low")) %>%
  pivot_wider(names_from = "typicality", values_from = "adj") %>%
  select(diff, noun, high, low) %>%
  arrange(diff) %>%
  ungroup() %>%
  # slice(1:10) %>%
  slice(1:6) %>%
  select(-diff) %>%
  rename("typical adjective" = "high",
         "atypical adjective" = "low") %>%
  xtable(caption = "The top six cases in which Wikipedia-trained word2vec similarities were worst at predicting human typicality judgments. In each case, word2vec judged the low-typicality adjective to be more similar to the noun than the high-typicality adjective.",
         label = "tab:pairs_tab")

print(pair_tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      include.rownames = FALSE)

```

# General Discussion

Language provides children a rich source of information about the world. However, this information is not always transparently available: because language is used to comment on the atypical, it does not perfectly mirror the world. Among adult conversational partners whose world knowledge is well-aligned, this principle allows people to converse informatively and avoid redundancy. But between a child and caregiver whose world knowledge is asymmetric, this pressure competes with other demands: what is minimally informative to an adult may be misleading to a child. Our results show that this pressure structures language to create a peculiar learning environment, one in which caregivers predominantly point out the atypical features of things. 

How, then, do children learn about the typical features of things? While younger children may gain an important foothold from hearing more description of typical features, they still face language dominated by atypical description. When we looked at more nuanced ways of extracting information from language (which may or may not be available to the developing learner), we found that models of distributional semantics capture little typical feature information.

Of course, perceptual information from the world may simplify this problem. In many cases, perceptual information may swamp information from language; children likely see enough orange carrots in the world to outweigh hearing "purple carrot.” It remains unclear, however, how children learn about categories for which they have scarcer evidence. Indeed, language information likely swamps perceptual information for many other categories, such as abstract concepts or those that cannot be learned about by direct experience. If such concepts pattern similarly to the concrete objects analyzed here, children are in a particularly difficult bind. 

It is also possible that other cues from language and interaction provide young learners with clues to what is typical or atypical, and these cues are uncaptured by our measure of usage statistics. Caregivers may highlight when a feature is typical by using certain syntactic constructions, such as generics (e.g., "tomatoes are red"). Caregivers may also mark the atypicality of a feature, for example demonstrating surprise. Such cues from language and the interaction may provide key information in some cases; however, given the sheer frequency of atypical descriptors, it seems unlikely that they are consistently well-marked.

Another possibility is that children expect language to be used informatively at a young age. Under this hypothesis, their language environment is not misleading at all, even without additional cues from caregivers. Children as young as two years old tend to use words to comment on what is new rather than what is known or assumed [@baker1988]. Children may therefore expect adjectives to comment on surprising features of objects. If young children expect adjectives to mark atypical features [@horowitz_childrens_2016], they can use description and the lack thereof to learn more about the world. We propose to investigate this question in Chapter 3.

Across our analyses, language is used with remarkable consistency: people talk about the atypical. Though parents might reasonably be broadly over-informative in order to teach their children about the world, this is not the case. This presents a potential puzzle for young learners who have limited world knowledge and limited pragmatic inferential abilities. Perceptual information and nascent pragmatic abilities may help fill in the gaps, but much remains to be explored to link these explanations to actual learning. Communication pressures are pervasive forces structuring the language children hear, and future work must disentangle whether children capitalize on them or are misled by them in learning about the world.
